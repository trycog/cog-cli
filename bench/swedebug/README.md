# SWE-bench Debug Benchmark

Measures whether cog's interactive debugger (`cog_debug_*` MCP tools) helps solve **real-world bugs** from [SWE-bench Verified](https://huggingface.co/datasets/princeton-nlp/SWE-bench_Verified) — actual GitHub issues from projects like Django, sympy, and scikit-learn.

Unlike `bench/debug/` which uses synthetic bugs, these are genuine issues where a debugger may provide meaningful advantage through runtime state inspection, breakpoints, and stepping.

## Structure

- **5 hand-picked tasks** from SWE-bench Verified (debugger-friendly: wrong runtime behavior, state corruption, edge cases)
- **Docker per task** for correct Python version + dependencies
- **2 variants** per task: `debug` (cog_debug_* MCP) vs `traditional` (Read/Grep/Edit/Bash)
- **Bind-mount architecture**: repo lives on host (Claude edits files directly), Docker provides Python env for running tests
- **Verification via pytest**: FAIL_TO_PASS tests must pass after fix, PASS_TO_PASS must not regress

## Architecture

```
Host (macOS/Linux)                    Docker Container
┌─────────────────────┐              ┌──────────────────┐
│ workspace/task-01/  │──bind-mount──│ /testbed/        │
│   (repo source)     │              │   (same files)   │
│                     │              │                  │
│ Claude edits files  │              │ pytest runs here │
│ cog debugger here   │              │ correct Python   │
└─────────────────────┘              └──────────────────┘
```

- **Repo files live on host** at `workspace/task-NN/`
- **Docker container** bind-mounts at `/testbed` — provides correct Python + deps
- **Claude edits files** on host via standard Read/Edit tools
- **Debug variant**: cog debugger launches Python on host, reads bind-mounted files
- **Both variants**: test verification uses `docker exec ... pytest`

## Quick Start

### 1. Select tasks (one-time)

```bash
pip install datasets
python3 bench/swedebug/select_tasks.py
```

Review the candidates and pick 5 debugger-friendly tasks. Add them to `tasks.json` with all required fields (see schema below).

### 2. Setup

```bash
bash bench/swedebug/setup.sh
```

This clones repos, builds Docker images, generates prompts, and configures Claude.

### 3. Run benchmarks

```bash
# Run all tasks, all variants
bash bench/swedebug/run.sh

# Run specific task(s)
bash bench/swedebug/run.sh 1 debug
bash bench/swedebug/run.sh '1 2 3' 'debug traditional'
```

### 4. View results

```bash
open bench/swedebug/dashboard.html
```

## Directory Layout

```
bench/swedebug/
├── README.md
├── select_tasks.py           # One-time: find candidates from HuggingFace
├── tasks.json                # 5 task definitions (populate after select_tasks.py)
├── setup.sh                  # Clone repos, build Docker, configure Claude
├── run.sh                    # Orchestrate: reset → claude -p → verify → record
├── collect.sh                # Aggregate .bench/*.json → dashboard.html
├── dashboard.html            # D3.js visualization
├── swedebug.md               # Generated by setup.sh: 5 tasks × 2 variants
├── docker/                   # Generated by setup.sh: per-task Dockerfiles
│   ├── task-01.Dockerfile
│   └── ...
├── .bench/                   # Result JSON files
└── workspace/                # Created by setup.sh (gitignored)
    ├── task-01/              # Repo checkout at buggy commit
    └── ...
```

## tasks.json Schema

```json
[{
  "id": 1,
  "instance_id": "django__django-NNNNN",
  "repo": "django/django",
  "repo_url": "https://github.com/django/django.git",
  "base_commit": "<sha>",
  "python_version": "3.11",
  "name": "Short description of the bug",
  "problem_statement": "Full GitHub issue text...",
  "test_cmd": "python -m pytest tests/path/to/test.py::TestClass::test_method -xvs",
  "fail_to_pass": ["tests/path/to/test.py::TestClass::test_method"],
  "pass_to_pass": ["tests/path/to/test.py::TestClass::test_other"],
  "install_cmd": "pip install -e .",
  "test_patch": "<unified diff that adds the failing test>",
  "gold_patch": "<unified diff of the correct fix>"
}]
```

## Metrics

Each test result records:
- `calls`: number of tool invocations
- `rounds`: number of LLM round-trips
- `cost_usd`: total API cost
- `duration_ms`: wall clock time
- `input_tokens` / `output_tokens`: token usage
- `verified`: whether FAIL_TO_PASS tests pass after fix
- `regression_free`: whether PASS_TO_PASS tests still pass

## Task Selection Criteria

Good candidates from SWE-bench Verified for debugger benchmarking:
- **Single-file patches** (one `diff --git` block) — keeps scope manageable
- **Patch size < 2000 chars** — bug fix, not feature addition
- **Clear problem statement** (> 40 words) — enough context for diagnosis
- **Runtime behavior bugs** — wrong output, crashes, edge cases (not missing features)
- **From well-supported repos** — Django, sympy, scikit-learn

## Reset Mechanism

Before each benchmark run, the workspace is reset to the base state:
```bash
git checkout .    # Restore all files to committed state
git clean -fd     # Remove any untracked files
```

The base state has the test patch applied (so failing tests exist) but not the fix.
